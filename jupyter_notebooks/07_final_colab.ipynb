{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5415ffc3",
            "metadata": {},
            "source": [
                "# 07 â€“ Final Stacking Model (Colab Ready)\n",
                "\n",
                "This notebook implements the complete, optimized pipeline for the Loan Prediction task. It is designed to be run on Google Colab with the full dataset.\n",
                "\n",
                "**Key Features:**\n",
                "- **Target**: Predicting **Probability of Default** (1 = Default, 0 = Paid Back).\n",
                "- **Clipping**: Handles extreme outliers in income and DTI.\n",
                "- **Feature Engineering**: Adds `loan_to_income`, `monthly_debt`, `interest_burden`.\n",
                "- **Hyperparameter Tuning**: Uses Optuna to find best params for XGBoost and LightGBM.\n",
                "- **Stacking**: Combines XGBoost, LightGBM, CatBoost, and **MLPClassifier (Neural Network)** using a Logistic Regression meta-learner."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install_deps",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "!pip install optuna catboost xgboost lightgbm scikit-learn pandas numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import optuna\n",
                "from functools import partial\n",
                "from pathlib import Path\n",
                "\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
                "from sklearn.ensemble import StackingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "\n",
                "from xgboost import XGBClassifier\n",
                "from lightgbm import LGBMClassifier\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "# Suppress warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Assuming data is uploaded to Colab or mounted from Drive\n",
                "# If using Colab upload, uncomment below:\n",
                "# from google.colab import files\n",
                "# uploaded = files.upload()\n",
                "\n",
                "DATA_DIR = Path('data') # Adjust this path if needed for Colab (e.g., '/content/drive/MyDrive/...')\n",
                "train_path = DATA_DIR / 'train.csv'\n",
                "test_path = DATA_DIR / 'test.csv'\n",
                "\n",
                "print(\"Loading data...\")\n",
                "train_df = pd.read_csv(train_path)\n",
                "test_df = pd.read_csv(test_path)\n",
                "print(f\"Train shape: {train_df.shape}\")\n",
                "print(f\"Test shape: {test_df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature_eng",
            "metadata": {},
            "outputs": [],
            "source": [
                "def feature_engineering(df):\n",
                "    df = df.copy()\n",
                "    # Clipping (99th percentile)\n",
                "    for col in ['annual_income', 'debt_to_income_ratio']:\n",
                "        if col in df.columns:\n",
                "            limit = df[col].quantile(0.99)\n",
                "            df[col] = df[col].clip(upper=limit)\n",
                "        \n",
                "    df['loan_to_income'] = df['loan_amount'] / (df['annual_income'] + 1)\n",
                "    df['monthly_debt'] = (df['annual_income'] / 12) * df['debt_to_income_ratio']\n",
                "    df['interest_burden'] = df['loan_amount'] * (df['interest_rate'] / 100)\n",
                "    \n",
                "    # Note: We decided NOT to include disposable_income and credit_income_interaction \n",
                "    # as they didn't improve the score in our tests.\n",
                "    return df\n",
                "\n",
                "print(\"Applying feature engineering...\")\n",
                "X_train_full = feature_engineering(train_df)\n",
                "X_test_full = feature_engineering(test_df)\n",
                "\n",
                "target_col = 'loan_paid_back'\n",
                "drop_cols = [target_col, 'id']\n",
                "\n",
                "# Drop target and id from training data\n",
                "X = X_train_full.drop(columns=[c for c in drop_cols if c in X_train_full.columns])\n",
                "\n",
                "# INVERT TARGET: 1 = Default, 0 = Paid Back\n",
                "y = 1 - X_train_full[target_col]\n",
                "print(\"Target inverted: predicting Probability of Default.\")\n",
                "\n",
                "# Drop id from test data (target doesn't exist there)\n",
                "X_test = X_test_full.drop(columns=['id'], errors='ignore')\n",
                "\n",
                "# Ensure columns match exactly\n",
                "X_test = X_test[X.columns]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocessor",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessor for Tree Models (Ordinal Encoding for Grades)\n",
                "def get_tree_preprocessor(X):\n",
                "    log_features = ['annual_income']\n",
                "    numeric_features = ['debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate',\n",
                "                        'loan_to_income', 'monthly_debt', 'interest_burden']\n",
                "    ordinal_features = ['grade_subgrade']\n",
                "    categorical_features = ['loan_purpose', 'gender', 'marital_status', 'education_level', 'employment_status']\n",
                "    \n",
                "    log_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='median')),\n",
                "        ('log', FunctionTransformer(np.log1p, validate=False)),\n",
                "        ('scaler', StandardScaler())\n",
                "    ])\n",
                "    \n",
                "    numeric_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='median')),\n",
                "        ('scaler', StandardScaler())\n",
                "    ])\n",
                "    \n",
                "    grades = sorted(X['grade_subgrade'].unique())\n",
                "    ordinal_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "        ('ordinal', OrdinalEncoder(categories=[grades], handle_unknown='use_encoded_value', unknown_value=-1))\n",
                "    ])\n",
                "    \n",
                "    categorical_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
                "    ])\n",
                "    \n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('log', log_transformer, log_features),\n",
                "            ('num', numeric_transformer, numeric_features),\n",
                "            ('ord', ordinal_transformer, ordinal_features),\n",
                "            ('cat', categorical_transformer, categorical_features)\n",
                "        ]\n",
                "    )\n",
                "    return preprocessor\n",
                "\n",
                "# Preprocessor for Neural Network (One-Hot Encoding for EVERYTHING, Standard Scaling)\n",
                "def get_nn_preprocessor(X):\n",
                "    log_features = ['annual_income']\n",
                "    numeric_features = ['debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate',\n",
                "                        'loan_to_income', 'monthly_debt', 'interest_burden']\n",
                "    # For NN, we treat grade_subgrade as categorical to One-Hot Encode it\n",
                "    categorical_features = ['grade_subgrade', 'loan_purpose', 'gender', 'marital_status', 'education_level', 'employment_status']\n",
                "    \n",
                "    log_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='median')),\n",
                "        ('log', FunctionTransformer(np.log1p, validate=False)),\n",
                "        ('scaler', StandardScaler())\n",
                "    ])\n",
                "    \n",
                "    numeric_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='median')),\n",
                "        ('scaler', StandardScaler())\n",
                "    ])\n",
                "    \n",
                "    categorical_transformer = Pipeline(steps=[\n",
                "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
                "    ])\n",
                "    \n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('log', log_transformer, log_features),\n",
                "            ('num', numeric_transformer, numeric_features),\n",
                "            ('cat', categorical_transformer, categorical_features)\n",
                "        ]\n",
                "    )\n",
                "    return preprocessor\n",
                "\n",
                "tree_preprocessor = get_tree_preprocessor(X)\n",
                "nn_preprocessor = get_nn_preprocessor(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "tuning",
            "metadata": {},
            "outputs": [],
            "source": [
                "# OPTIONAL: Run Tuning on Colab (takes time)\n",
                "# Set RUN_TUNING = True to re-optimize on the full dataset (recommended for best results)\n",
                "RUN_TUNING = True\n",
                "\n",
                "def objective_xgb(trial, X, y, preprocessor):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
                "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
                "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
                "        'random_state': 42,\n",
                "        'n_jobs': -1,\n",
                "        'eval_metric': 'auc'\n",
                "    }\n",
                "    model = XGBClassifier(**params)\n",
                "    pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])\n",
                "    # Use 3-fold CV on a subset for speed during tuning, or full data if patient\n",
                "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
                "    scores = cross_val_score(pipeline, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
                "    return scores.mean()\n",
                "\n",
                "def objective_lgbm(trial, X, y, preprocessor):\n",
                "    params = {\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
                "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
                "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
                "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
                "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
                "        'random_state': 42,\n",
                "        'n_jobs': -1,\n",
                "        'verbose': -1\n",
                "    }\n",
                "    model = LGBMClassifier(**params)\n",
                "    pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])\n",
                "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
                "    scores = cross_val_score(pipeline, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
                "    return scores.mean()\n",
                "\n",
                "if RUN_TUNING:\n",
                "    print(\"Starting Hyperparameter Tuning (this may take a while)...\")\n",
                "    # Use a larger subset for better tuning on Colab\n",
                "    X_tune = X.sample(n=50000, random_state=42)\n",
                "    y_tune = y.loc[X_tune.index]\n",
                "    \n",
                "    study_xgb = optuna.create_study(direction='maximize')\n",
                "    study_xgb.optimize(partial(objective_xgb, X=X_tune, y=y_tune, preprocessor=tree_preprocessor), n_trials=20)\n",
                "    best_xgb_params = study_xgb.best_params\n",
                "    print(f\"Best XGB params: {best_xgb_params}\")\n",
                "    \n",
                "    study_lgbm = optuna.create_study(direction='maximize')\n",
                "    study_lgbm.optimize(partial(objective_lgbm, X=X_tune, y=y_tune, preprocessor=tree_preprocessor), n_trials=20)\n",
                "    best_lgbm_params = study_lgbm.best_params\n",
                "    print(f\"Best LGBM params: {best_lgbm_params}\")\n",
                "else:\n",
                "    # Fallback to params found in local testing (tuned on 10k rows)\n",
                "    print(\"Using pre-tuned parameters...\")\n",
                "    best_xgb_params = {'n_estimators': 152, 'learning_rate': 0.045, 'max_depth': 5, 'subsample': 0.74, 'colsample_bytree': 0.92, 'reg_alpha': 8.43, 'reg_lambda': 6.62}\n",
                "    best_lgbm_params = {'n_estimators': 126, 'learning_rate': 0.049, 'num_leaves': 39, 'feature_fraction': 0.76, 'bagging_fraction': 0.87, 'bagging_freq': 5}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "stacking",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Stacking Classifier on FULL dataset...\")\n",
                "\n",
                "best_xgb = XGBClassifier(**best_xgb_params, random_state=42, n_jobs=-1, eval_metric='auc')\n",
                "best_lgbm = LGBMClassifier(**best_lgbm_params, random_state=42, n_jobs=-1, verbose=-1)\n",
                "cat_clf = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=6, random_state=42, verbose=0, allow_writing_files=False)\n",
                "mlp_clf = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='adaptive', learning_rate_init=0.001, max_iter=200, random_state=42, early_stopping=True)\n",
                "\n",
                "pipe_xgb = Pipeline([('preprocessor', tree_preprocessor), ('model', best_xgb)])\n",
                "pipe_lgbm = Pipeline([('preprocessor', tree_preprocessor), ('model', best_lgbm)])\n",
                "pipe_cat = Pipeline([('preprocessor', tree_preprocessor), ('model', cat_clf)])\n",
                "pipe_mlp = Pipeline([('preprocessor', nn_preprocessor), ('model', mlp_clf)])\n",
                "\n",
                "estimators = [\n",
                "    ('xgb', pipe_xgb),\n",
                "    ('lgbm', pipe_lgbm),\n",
                "    ('cat', pipe_cat),\n",
                "    ('mlp', pipe_mlp)\n",
                "]\n",
                "\n",
                "stacking_clf = StackingClassifier(\n",
                "    estimators=estimators,\n",
                "    final_estimator=LogisticRegression(),\n",
                "    cv=5,\n",
                "    n_jobs=-1,\n",
                "    passthrough=False\n",
                ")\n",
                "\n",
                "stacking_clf.fit(X, y)\n",
                "print(\"Training Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "submission",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Generating predictions...\")\n",
                "test_probabilities = stacking_clf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "submission = pd.DataFrame({\n",
                "    'id': test_df['id'],\n",
                "    'loan_default': test_probabilities\n",
                "})\n",
                "\n",
                "submission.to_csv('final_submission_colab.csv', index=False)\n",
                "print(\"Saved submission to final_submission_colab.csv\")\n",
                "\n",
                "# If using Colab, trigger download\n",
                "# from google.colab import files\n",
                "# files.download('final_submission_colab.csv')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}