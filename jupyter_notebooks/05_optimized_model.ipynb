{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5415ffc3",
            "metadata": {},
            "source": [
                "# 05 â€“ Optimized Model with EDA Insights"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b98c7184",
            "metadata": {},
            "source": [
                "This notebook implements the optimized preprocessing strategy derived from EDA:\n",
                "- **Log-transform** `annual_income` to handle skew.\n",
                "- **Ordinal Encode** `grade_subgrade` to preserve rank information.\n",
                "- **Keep outliers** as tree-based models handle them well.\n",
                "- **One-Hot Encode** other categorical features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af2b5a8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.append(os.path.abspath(\"..\"))\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
                "\n",
                "DATA_DIR = Path('../data')\n",
                "train_path = DATA_DIR / 'train.csv'\n",
                "test_path = DATA_DIR / 'test.csv'\n",
                "\n",
                "train_df = pd.read_csv(train_path)\n",
                "test_df = pd.read_csv(test_path)\n",
                "\n",
                "# Sample for faster iteration if needed, but we use full data for best results or large sample\n",
                "# train_df = train_df.sample(n=10000, random_state=42)\n",
                "train_df.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "329483b5",
            "metadata": {},
            "source": [
                "## Feature Engineering and Preprocessing\n",
                "\n",
                "We define specific transformers for different column types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fa876b89",
            "metadata": {},
            "outputs": [],
            "source": [
                "target_col = 'loan_paid_back'\n",
                "X = train_df.drop(columns=[target_col])\n",
                "y = train_df[target_col]\n",
                "\n",
                "# Define column groups\n",
                "log_features = ['annual_income']\n",
                "numeric_features = ['debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\n",
                "ordinal_features = ['grade_subgrade']\n",
                "categorical_features = ['loan_purpose', 'gender', 'marital_status', 'education_level', 'employment_status']\n",
                "\n",
                "# 1. Log Transformer for annual_income\n",
                "log_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('log', FunctionTransformer(np.log1p, validate=False)),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# 2. Standard Numeric Transformer\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# 3. Ordinal Transformer for grade_subgrade\n",
                "# We need to define the order. A1 is best (highest prob of payback?), F5 is worst.\n",
                "# Let's check unique values to be sure, or just rely on alphabetical sort which works for A1..F5\n",
                "# Alphabetical: A1, A2, ... F5. \n",
                "# If A1 is 'better' (higher credit), we might want it to have higher value? \n",
                "# Or just mapping them to 0..N is fine for trees. \n",
                "# Let's use alphabetical order.\n",
                "grades = sorted(X['grade_subgrade'].unique())\n",
                "ordinal_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('ordinal', OrdinalEncoder(categories=[grades], handle_unknown='use_encoded_value', unknown_value=-1))\n",
                "])\n",
                "\n",
                "# 4. Categorical Transformer\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('log', log_transformer, log_features),\n",
                "        ('num', numeric_transformer, numeric_features),\n",
                "        ('ord', ordinal_transformer, ordinal_features),\n",
                "        ('cat', categorical_transformer, categorical_features)\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00bb62e6",
            "metadata": {},
            "source": [
                "## Model Selection\n",
                "\n",
                "We test GradientBoosting and RandomForest as they are likely to benefit from the ordinal encoding and robust to the remaining noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4d0c4fd",
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
                "    'RandomForest': RandomForestClassifier(\n",
                "        n_estimators=300,\n",
                "        max_depth=10,  # Limit depth to prevent overfitting given the noise\n",
                "        min_samples_leaf=5,\n",
                "        random_state=42\n",
                "    ),\n",
                "    'GradientBoosting': GradientBoostingClassifier(\n",
                "        n_estimators=200,\n",
                "        learning_rate=0.1,\n",
                "        max_depth=5,\n",
                "        random_state=42\n",
                "    )\n",
                "}\n",
                "\n",
                "results = []\n",
                "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "# Use a subset for quick validation if dataset is huge, else full\n",
                "# X_sample = X.sample(10000, random_state=42)\n",
                "# y_sample = y.loc[X_sample.index]\n",
                "X_train_eval = X\n",
                "y_train_eval = y\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"Training {name}...\")\n",
                "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
                "    scores = cross_val_score(pipeline, X_train_eval, y_train_eval, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
                "    results.append({\n",
                "        'model': name,\n",
                "        'mean_auc': scores.mean(),\n",
                "        'std_auc': scores.std(),\n",
                "    })\n",
                "    print(f\"{name} AUC: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values('mean_auc', ascending=False).reset_index(drop=True)\n",
                "results_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58b13bfa",
            "metadata": {},
            "outputs": [],
            "source": [
                "best_model_name = results_df.loc[0, 'model']\n",
                "best_model = models[best_model_name]\n",
                "print(f\"Best model: {best_model_name}\")\n",
                "\n",
                "# Fit on full data\n",
                "best_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', best_model)])\n",
                "best_pipeline.fit(X, y)\n",
                "\n",
                "# Predict on test\n",
                "test_probabilities = best_pipeline.predict_proba(test_df)[:, 1]\n",
                "submission = pd.DataFrame({\n",
                "    'id': test_df['id'],\n",
                "    'loan_paid_back': test_probabilities\n",
                "})\n",
                "submission.to_csv('data/optimized_submission.csv', index=False)\n",
                "print(\"Saved submission to data/optimized_submission.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}