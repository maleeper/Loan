{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5415ffc3",
   "metadata": {},
   "source": [
    "# 04 â€“ Best Model via AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c7184",
   "metadata": {},
   "source": [
    "This notebook trains several classification models using the provided loan dataset and\n",
    "selects the best-performing model based on the Area Under the ROC Curve (AUC).\n",
    "Both the training and test CSV files are utilized so that the final, best model can\n",
    "produce predictions for the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40ae58",
   "metadata": {},
   "source": [
    "The training data includes the `loan_paid_back` target column that we want to model.\n",
    "The test data shares the same feature columns (minus the target), which we will use\n",
    "for generating predictions once the best model is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(train_df['loan_paid_back'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329483b5",
   "metadata": {},
   "source": [
    "## Feature engineering and preprocessing\n",
    "\n",
    "We separate the target (`loan_paid_back`) from the predictor columns. Numerical\n",
    "features will be imputed with the median and scaled, while categorical features will\n",
    "be imputed with the most frequent value and one-hot encoded. This combined preprocessing\n",
    "pipeline keeps the feature transformations consistent across any model we evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa876b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'loan_paid_back'\n",
    "X = train_df.drop(columns=[target_col])\n",
    "y = train_df[target_col]\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb62e6",
   "metadata": {},
   "source": [
    "## Model selection using AUC\n",
    "\n",
    "We benchmark several commonly used classification algorithms, each wrapped in a pipeline\n",
    "that applies the preprocessing steps defined above. Using stratified 5-fold cross-validation\n",
    "helps provide a robust estimate of each model's performance. The model with the highest\n",
    "mean AUC across folds is selected as the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    scores = cross_val_score(pipeline, X, y, cv=skf, scoring='roc_auc')\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'mean_auc': scores.mean(),\n",
    "        'std_auc': scores.std(),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('mean_auc', ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b13bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.loc[0, 'model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"Best model based on CV AUC: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ac7b1",
   "metadata": {},
   "source": [
    "## Fit the best model on the full training data and score the test set\n",
    "\n",
    "We now refit the pipeline containing the best-performing model on all available training data.\n",
    "The resulting estimator is then used to generate the probability of `loan_paid_back = 1`\n",
    "for every row in the test data. These probabilities can be used for downstream evaluation\n",
    "or submission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', best_model)])\n",
    "best_pipeline.fit(X, y)\n",
    "\n",
    "test_probabilities = best_pipeline.predict_proba(test_df)[:, 1]\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'loan_paid_back': test_probabilities\n",
    "})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('data/best_model_submission.csv', index=False)\n",
    "print('Saved predictions to data/best_model_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
